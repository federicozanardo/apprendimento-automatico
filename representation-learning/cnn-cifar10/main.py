# -*- coding: utf-8 -*-
"""CNN per CIFAR-10

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_s9-rXYrhqSf06J_-OrbmFyTTCEHrp0p

# Convolutional Neural Network per il dataset CIFAR-10

## Introduzione

Una **rete neurale convoluzionale** (**CNN**) è un tipo di rete neurale artificiale *feed-forward*. Queste tipologie di reti sono variazioni di *percettroni multistrato* progettate per minimizzare la fase di preprocessing.

L’idea alla base di una rete neurale convoluzionale è quella di applicare un filtro alle immagini prima che queste vengano processate dalla **rete neurale profonda** (**deep neural network**).

Una **convoluzione** è un filtro che viene applicato su un'immagine, la elabora ed estrae delle feature dalle caratteristiche comuni. L'applicazione del filtro consente quindi di mettere in rilievo i dettagli delle immagini in modo tale che sia possibile ricavarne il significato ed identificare gli oggetti. Concretamente, un filtro consiste in un insieme di moltiplicatori.

Il **pooling** è un'altra classe di filtri in cui i pixel sono raggruppati e filtrati in un sottoinsieme. L'applicazione di questo filtro permette di ridurre le dimensioni a un quarto di quelle originali, pur mantenendo le caratteristiche iniziali. I valori da applicare al filtro sono dei parametri che vengono appressi nel processo di apprendimento dei dati di training. Quando l'immagine entra nel livello convoluzionale, un certo numero di filtri, inizializzati casualmente, viene applicato in modo da produrre un risultato che verrà passato al secondo livello della rete come input. Questo procedimento si ripete finché la rete non individua quei valori dei filtri tali per cui il matching tra l’immagine in input e il valore predetto produca meno errori possibili. Questo processo si chiama **feature extraction**. Tale classe di filtri è utile in quanto permette di risolvere una limitazione degli strati convoluzionali. Quest'ultimi registrano la posizione precisa delle feature dell'immagine di input. Di conseguenza la rete tenderà ad andare in *overfitting*, non potendo creare così un modello generalizzato.

Nelle reti realizzate in questo notebook, l'ultimo strato utilizza la *funzione di attivazione* **SoftMax**. Così facendo, si ottiene in output un range di valori che va da 0 a 1 e che rappresenta la probabilitá di quella predizione.

## Implementazione della rete

### Analisi del dataset

Includo il dataset e tutte le librerie necessarie.
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

from tensorflow import keras
from tensorflow.keras.datasets import cifar10
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Activation, Dropout, MaxPooling2D
from tensorflow.keras.utils import plot_model
import matplotlib.pyplot as plt

"""Preparo i dati di train e di test."""

(X_train, y_train), (X_test, y_test) = cifar10.load_data()

"""Memorizzo le classi di immagini presenti nel dataset."""

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

"""Ottengo alcune informazioni riguardo il dataset. É possibile vedere che il dataset è costituito da 6000 immagini a colori (RGB) di 32×32 pixel."""

X_train.shape

len(y_train)

y_train

X_test.shape

len(y_test)

"""### Preprocessing

Osservo un'immagine dall'insieme di training. I pixel delle immagini hanno valori che cadono nell'intervallo da 0 a 255.
"""

plt.figure()
plt.imshow(X_train[43])
plt.colorbar()
plt.grid(False)
plt.show()

"""Questi valori vanno scalati in un intervallo tra 0 e 1 prima di poterli utilizzare per la realizzazione della rete. Per fare ciò, si dividono i valori per 255. È importante che l'insieme di train e l'insieme di test vengano preprocessati in ugual modo."""

X_train = X_train / 255.0
X_test = X_test / 255.0

"""Visualizzo le prime 20 immagini dall'insieme di train e applico per ogni immagine il nome della relativa classe."""

plt.figure(figsize=(10,10))
for i in range(20):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(X_train[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[(y_train[i])[0]])
plt.show()

"""### Costruzione della rete

Di seguito si descrivono i vari strati della rete convoluzionale:
- **Input**: in questo caso, il modello prende come input delle immagini di 32×32 pixel RGB;
- **Strati convoluzionali**: l'architettura prevede lo l'utilizzo di strati convoluzionali con piccoli filtri 3×3 seguiti da uno strato MaxPooling;
- **Strati fully-connected**: il modello in questione ha tre livelli completamente connessi;
- **Strati nascosti**: tutti gli strati nascosti del modello utilizzano la funazione di attivazione ReLU.

Definisco quali sono i livelli utilizzati nella rete e i relativi parametri.
"""

model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
model.add(Dense(10, activation='softmax'))

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Show the layers of the net
plot_model(model, show_layer_names=False, show_shapes=True)

"""Eseguo l'addestramento della rete."""

history = model.fit(X_train, y_train, batch_size=64, epochs=15, validation_data=(X_test, y_test))

"""Calcolo l'accuratezza della rete."""

test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)

print('\nTest accuracy:', test_acc)

"""Mostro graficamente i risultati ottenuti."""

# plot loss
plt.subplot(211)
plt.title('Cross Entropy Loss')
plt.plot(history.history['loss'], color='blue', label='train')
plt.plot(history.history['val_loss'], color='orange', label='test')

# plot accuracy
plt.subplot(212)
plt.title('Classification Accuracy')
plt.plot(history.history['accuracy'], color='blue', label='train')
plt.plot(history.history['val_accuracy'], color='orange', label='test')

plt.legend()
plt.show()

"""### Ottimizzazione della rete

Il modello appena descritto può essere ottimizzato introducendo una **regolarizzazione Dropout**. Il Dropout è una tecnica che elimina casualmente dei nodi dalla rete. Ha un effetto regolarizzante in quanto i nodi rimanenti devono adattarsi per riprendere l'allentamento dei nodi rimossi.
Dropout può essere introdotto nel modello aggiungendo dei nuovi strati, dove la quantità dei nodi rimossi è specificata come parametro.

Definisco quali sono i livelli utilizzati nella rete e i relativi parametri, aggiungendo degli strati Dropout.
"""

model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Show the layers of the net
plot_model(model, show_layer_names=False, show_shapes=True)

"""Eseguo l'addestramento della rete."""

history = model.fit(X_train, y_train, batch_size=64, epochs=15, validation_data=(X_test, y_test))

"""Calcolo l'accuratezza della rete."""

test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)

print('\nTest accuracy:', test_acc)

"""Mostro graficamente i risultati ottenuti."""

# plot loss
plt.subplot(211)
plt.title('Cross Entropy Loss')
plt.plot(history.history['loss'], color='blue', label='train')
plt.plot(history.history['val_loss'], color='orange', label='test')

# plot accuracy
plt.subplot(212)
plt.title('Classification Accuracy')
plt.plot(history.history['accuracy'], color='blue', label='train')
plt.plot(history.history['val_accuracy'], color='orange', label='test')

plt.legend()
plt.show()