# -*- coding: utf-8 -*-
"""Word2Vec per IMDB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F0VBuKBiQ3Efg2a1KkKJFwXtqr9ChvCo

# Word2Vec per IMDB

## Upload ed esplorazione del dataset

Includo le librerie necessarie per la fase di preprocessing e per l'implementazione del modello.
"""

import pandas as pd
import matplotlib.pyplot as plt

from gensim.models import word2vec

import nltk
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')

"""Si effettui l'upload del dataset."""

train_set = pd.read_csv("labeledTrainData.tsv", header=0, delimiter="\t")
test_set = pd.read_csv("testData.tsv", header=0, delimiter="\t")

"""Si osservino alcuni esempi dall'insieme di training."""

train_set

"""Si osservi il primo elemento dall'insieme di test."""

test_set['review'][0]

"""## Preprocessing

In questa fase cerco di eliminare tutti gli elementi che possono non utili ai fini dell'implementazione del modello e della predizione. Si cerca di tenere soltanto quelle parole che contengono un vero significato.

Suddivido il testo di ogni recensione in frasi.
"""

tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')

sentences = []
for review in train_set['review']:
    sentences += tokenizer.tokenize(review.strip())

sentences[0]

"""Suddivido ogni frase di ogni recensione in una lista di parole (*token*)."""

words = []
for sentence in sentences:
  tokens = word_tokenize(sentence)

  new_tokens = []
  for token in tokens:
    if len(token) > 0:
      new_tokens.append(token.lower())
  
  words.append(new_tokens)

words[0]

"""Dall'insieme delle parole che costituiscono le frasi, si rimuovano le *stop word*, ovvero, quelle parole che non contribuiscono al significato più profondo della frase, ad esempio gli articoli."""

no_stop_words = []
for word in words:
    stops = set(stopwords.words("english"))     
    no_stop = [w for w in word if not w in stops]
    no_stop_words.append(no_stop) 
    
no_stop_words[0]

"""Si rimuova la punteggiatura dalle frasi in quanto non hanno alcuna rilevanza ai fini della realizzazione del modello."""

no_punctuation = []
for no_stop_word in no_stop_words:
  words = [word for word in no_stop_word if word.isalpha()]
  no_punctuation.append(words)

no_punctuation[0]

"""Si rimuovano inoltre le *stem words*. *Stemming* è il processo di riduzione di ogni parola alla sua radice o base. Ad esempio, "playing", "played", "player" si riducono tutti alla stessa radice "play"."""

porter_stemmer = PorterStemmer()
final_words = []

for only_words in no_punctuation:
  final_words.append([porter_stemmer.stem(word) for word in only_words])

final_words[0]

"""## Implementazione del modello

Si implementi un modello con un'architettura CBOW.

I parametri che sono stati utilizzati per l'implementazione del modello sono:
* `sg=0`: specifica se si vuole utilizzare l'architettura CBOW (0) o Skip-Gram (1);
* `min_count=30`: ignora tutte le parole con una frequenza totale inferiore a 30;
* `alpha=0.01`: learning rate iniziale;
* `workers=5`: usa un numero specifico di thread per la fase di training del modello;
* `window=2`: indica la distanza massima tra la parola corrente e quella prevista all'interno di una frase;
* `sample=1e-3`: è una soglia per configurare quali sono le parole a maggior frequenza sottocampionate casualmente;
* `size=300`: indica la dimensionalità del vettore.
"""

# Build the model
model = word2vec.Word2Vec(final_words, sg=0, min_count=30, alpha=0.01, 
                          workers=5, window=2, sample=1e-3, size=300, seed=42)

# To make the model memory efficient
model.wv.init_sims(replace=True)

"""Si mostrino quali sono le parole simili a *good*."""

model.wv.most_similar('good')

"""Si mostrino quali sono le parole simili a *radio*."""

model.wv.most_similar('radio')

"""Si osservi la similarità presente tra le parole *good* e *car*."""

model.wv.similarity("good", "car")

"""Si osservi la similarità presente tra le parole *drive* e *car*."""

model.wv.similarity("drive", "car")

"""È comprensibile che la parola *drive* sia più vicina (semanticamente e di conseguenza anche spazialmente) a *car* piuttosto che *good* a *car*.

Si osserivino altre similarità.
"""

model.wv.similarity("good", "bad")

model.wv.similarity("cat", "car")

"""Si osservi inoltre la capacità del modello di individure la parola che non ha una relazione semantica con le altre parole fornite in input. Nell'esempio illustrato, tra i nomi di animali inseriti, si è aggiunta la parola *car*, che come ci si aspetta, avrà una relazione semantica minore rispetto alla relazione semantica che hanno le altre parole tra di loro."""

model.wv.doesnt_match("cat dog car bird".split())