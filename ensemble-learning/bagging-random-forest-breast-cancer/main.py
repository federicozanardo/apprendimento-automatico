# -*- coding: utf-8 -*-
"""Bagging e RandomForset per il dataset Breast Cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_NvI3EKvWB-X60bA4krSC6d-IRP0TQBl

# **Bagging e RandomForset per il dataset Breast Cancer**

## Introduzione

### **Ensemble learning**

L'*ensemble learning* è un paradigma dell'apprendimento automatico dove molteplici modelli (detti *weak learner*) vengono addestrati per risolvere il medesimo problema e combinati per ottenere dei risulati migliori. L'ipotesi principale di questo paradigma è che quando i weak learner sono combinati correttamente, è possibile ottenere dei modelli più accurati.

La scelta del modello da utilizzare per risolvere un determinato problema è una decisione di fondamentale importanza per riuscire ad ottenere dei buoni risultati. Si può scegliere un modello piuttosto che un altro per vari motivi:
* quantità di dati a disposizione;
* dimensionalità dello spazio delle ipotesi $\mathcal{H}$;
* ...

Il quesito che ci si pone è perchè è conveniente, in certi casi, combinare dei modelli. Tale quesito viene analizzato secondo due approcci: un approccio *intuitivo* e un approccio *teorico*.

#### **Analisi intuitiva**

I motivi per cui ha senso combinare dei modelli ai fini di ottenere delle predizioni più accurate sono:
* **Statistico**: nel caso in cui non hanno a disposizione molto dati, diverse ipotesi possono avere lo stesso livello di accuratezza sui dati di training. Facendo la *media* delle ipotesi determinate che forniscono una *buona* predizione, è possibile ridurre il rischio di andare a scegliere un modello inadeguato. Di conseguenza, sarà più probabile trovare un modello che fornirà delle predizioni migliori;
* **Computazionale**: anche in presenza di un numero sufficiente di esempi di training, gli algoritmi di apprendimento possono rimanere bloccati negli *ottimi locali*. Un *insieme* (*ensemble*) costruito eseguendo la ricerca locale da differenti punti di partenza può fornire un'approssimazione migliore alla vera funzione sconosciuta;
* **Rappresentativo**: in certi casi, la vera funzione non può essere rappresentata da nessuna delle ipotesi presenti nello spazio delle ipotesi $\mathcal{H}$. Formando somme ponderate di ipotesi tratte da $\mathcal{H}$ è possibile espandere lo spazio delle funzioni rappresentabili.

#### **Analisi teorica**

Avere un *bias basso* e una *varianza bassa* (sebbene il più delle volte variano in direzioni opposte) sono le due caratteristiche fondamentali previste per un modello. In effetti, per essere in grado di risolvere un problema, si desidera che il modello abbia abbastanza gradi di libertà per risolvere la complessità dei dati sottostanti, ma si vuole anche che tale modello non abbia troppi gradi di libertà per evitare di avere una varianza elevata. Questo è noto come il *compromesso bias-varianza*.

Data $y = f(x) + \epsilon$ e un'ipotesi $g$, l'errore quadratico può essere *scomposto* in:
\begin{equation}
E[(y - g(x))^2] = rumore^2 + bias^2 + varianza
\end{equation}
dove:
* $rumore^2$: $E[(y - f(x))^2]$ è irriducibile;
* $bias^2$: $(E[g(x)] - f(x))^2$
* $varianza$: $E[(g(x) - E[g(x)])^2]$.

Solitamente, la media di più ipotesi riduce la varianza, ma può anche ridurre il bias.

### **Bagging**

*Bagging* sta per **bootstrap aggregating** e ha l'obiettivo di produrre un modello *ensemble* più robusto dei singoli weak learner che lo compongono.

Il *bootstrap* è una tecnica di ricampionamento utilizzata per stimare le statistiche su una popolazione, campionando un set di dati con sostituzione.

Sotto alcune ipotesi, i campioni così creati hanno delle proprietà statistiche piuttosto buone. Essi possono essere considerati dei campioni rappresentativi e indipendenti della vera distribuzione dei dati. Le ipotesi da verificare sono:
* la dimensione $N$ dell'insieme di dati iniziale dovrebbe essere abbastanza grande da catturare la maggior parte della complessità della distribuzione sottostante, in modo che il campionamento dall'insieme di dati sia una buona approssimazione del campionamento dalla distribuzione reale (**rappresentatività**);
* la dimensione $N$ dell'insieme di dati dovrebbe essere sufficientemente grande rispetto alla dimensione $B$ dei campioni bootstrap in modo che i campioni non siano troppo correlati (**indipendenza**).

Nella maggior parte dei casi, considerare campioni veramente indipendenti richiederebbero troppi dati rispetto alla quantità realmente disponibile. È possibile quindi utilizzare il bootstrap per generare diversi esempi di bootstrap che possono essere considerati *quasi rappresentativi* e *quasi indipendenti* (campioni *quasi i.i.d.*). Questi campioni bootstrap permettono di approssimare la varianza del modello, valutandone il valore per ciascuno di essi.

A causa della varianza teorica dell'insieme di training (tale insieme è un campione osservato da una distribuzione sottostante vera *sconosciuta*), anche il modello adattato è soggetto a variabilità. Se fosse stato considerato un altro insieme di training, avremmo ottenuto un modello diverso.

L'idea del **bagging** è quella di voler adattare diversi modelli indipendenti e *fare la media* delle loro predizioni per ottenere un modello con una varianza inferiore. Tuttavia, nella pratica, non è possibile adattare modelli completamente indipendenti perché richiederebbero troppi dati. Quindi, tenendo in considerazione le ipotesi del bootstrapping citate precedentemente, ci si concentrerà sulle proprietà approssimative dei campioni bootstrap (*rappresentatività* e *indipendenza*) per adattare i modelli che sono quasi indipendenti.

Per realizzare tutto ciò, ogni campione bootstrap diventa l'insieme di training di ogni weak learner. Tali weak learner venogno *aggregati* in modo tale da *fare la media* delle loro predizioni ed ottenere un modello con una varianza inferiore. Quindi, poichè i campioni sono i.i.d., lo sono anche i modelli addestrati su tali campioni. Di conseguenza, la *media* delle loro predizioni permette di ridurre la varianza.

Si assuma di avere $\mathcal{C}$ campioni bootstrap, ognuno di dimensione $B$. Siano $w_1(\cdot), \dots , w_{\mathcal{C}}(\cdot)$ i weak learner. La creazione dell'ensemble learner può essere definita ad esempio in questo modo: $s_{\mathcal{C}} = \frac{1}{\mathcal{C}} \sum_{i = 1}^{\mathcal{C}} w_i(\cdot)$.

A differenza del *boosting*, i weak learner di bagging e *stacking* sono considerati indipendenti l'uno dall'altro e pertanto è possibile *addestrarli contemporaneamente*.

### **RandomForest**

I *decision tree* sono dei weak learner molto utilizzati per realizzare un ensemble learner. Un ensemble learner composto da più decision tree viene chiamato *foresta*. I decision tree che compongono una foresta possono essere scelti per essere poco profondi o profondi. Gli alberi poco profondi hanno una varianza minore ma un bias più elevato. Gli alberi profondi, invece, hanno un bias basso ma una varianza elevata e quindi rilevanti per il bagging.

Le *random forest* utilizzano anche un'altra tecnica per rendere gli alberi adattati un po' meno correlati tra loro: al crescere di ogni albero, invece di campionare solo sulle osservazioni nell'insieme di dati iniziale per generare un campione bootstrap, si campiona anche sulle *feature*. Così facendo, si seleziona soltanto un sottoinsieme casuale di feature per la costruzione dell'albero.

### **Dataset**

Il dataset utilizzato in questo notebook viene fornito da sklearn. In alternativa, il dataset è reperibile al seguente link: [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29).

## Caricamento del dataset e delle librerie necessarie
"""

import numpy as np
import pandas as pd

# Dataset
from sklearn.datasets import load_breast_cancer

# Preprocessing
from sklearn.preprocessing import LabelEncoder

# Splitting
from sklearn.model_selection import train_test_split

# Feature scaling
from sklearn.preprocessing import StandardScaler

# Models
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

# Ensemble
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier

# Cross Validation
from sklearn.model_selection import GridSearchCV, KFold

# Metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score 
from sklearn.metrics import f1_score, classification_report, confusion_matrix

import matplotlib.pyplot as plt
import seaborn as sns
from tabulate import tabulate

dataset = load_breast_cancer()

"""## Esplorazione del dataset

Si ottenga una descrizione del dataset.
"""

print(dataset.DESCR)

df = pd.DataFrame(np.c_[dataset['data'], dataset['target']], columns = np.append(dataset['feature_names'], ['target']))

df.head()

"""## Preprocessing"""

X = df.iloc[:, df.columns != "target"].values
y = df.iloc[:, df.columns == "target"].values

"""Si codifichino i valori target tramite `LabelEncoder`."""

encoder = LabelEncoder()
y = encoder.fit_transform(y)

encoder.classes_

"""## Split del dataset

Si suddivida il dataset in training set e test set.
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Feature scaling

Si effettui uno scaling dei dati con `StandardScaler`.
"""

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""## Modelli per `BaggingClassifier`

Definisco una funzione che permette di calcolare alcune valutazioni per i modelli.
"""

def evaluate(y_test, y_pred):
  print("Accuracy score: ",   accuracy_score(y_test, y_pred))
  print("Precision score: ",  precision_score(y_test, y_pred))
  print("Recall score: ",     recall_score(y_test, y_pred))
  print("Report\n",           classification_report(y_test, y_pred))
  print("Confusion matrix\n", confusion_matrix(y_test, y_pred))

"""Di seguito si illustrino i modelli che verranno utilizzati come weak learner.

### **Decision Tree**
"""

dt = DecisionTreeClassifier(random_state=42)

dt.fit(X_train, y_train)

y_pred = dt.predict(X_test)

evaluate(y_test, y_pred)

"""### **K-Nearest Neighbors**"""

knn = KNeighborsClassifier()

knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

evaluate(y_test, y_pred)

"""### **Logistic Regression**"""

lgr = LogisticRegression(random_state=42, n_jobs=-1)

lgr.fit(X_train, y_train)

y_pred = lgr.predict(X_test)

evaluate(y_test, y_pred)

"""## Model selection

### **Model selection per i weak learner e per `RandomForest`**

Si definiscano i classificatori che verranno utilizzati nella model selection.
"""

cls_names = ["Decision Tree", 
             "K-Nearest Neighbors",
             "Logistic Regression",
             "RandomForest"] 

classifiers = [DecisionTreeClassifier,
               KNeighborsClassifier,
               LogisticRegression,
               RandomForestClassifier]

"""Si crei un dizionario di dizionari che contiene la griglia dei parametri per i classificatori."""

p_grid = {
    "dt"  : [{'max_depth': [None, 3, 5, 7, 9], 
              'splitter': ['best', 'random'], 
              'criterion': ['gini', 'entropy'], 
              'ccp_alpha': [0, 0.001, 0.01, 0.1]}],
    "knn" : [{'n_neighbors': [3, 5, 7, 9], 
              'weights': ['uniform', 'distance'], 
              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], 
              'leaf_size': [20, 30, 40], 
              'n_jobs': [-1]}],
    "lgr" : [{
              'C':[0.001, 0.01, 0.1, 1, 2, 4], 
              'max_iter':[100, 200, 500, 1000], 
              'warm_start': [True, False],
              'random_state': [42], 
              'n_jobs': [-1]
              }],
    "rfc" : [{'n_estimators': [20, 30, 50, 100, 150], 
              'criterion': ['gini', 'entropy'], 
              'warm_start': [False, True], 
              'ccp_alpha': [0, 0.001, 0.01, 0.1], 
              'random_state': [42], 
              'n_jobs': [-1]}]
}
p_grid

"""Si definisca K-Fold Cross Validation con K = 5."""

skf = KFold(n_splits=5, shuffle=False, random_state=42)

best_models, best_scores = [], []

for i, (name, clf, grid) in enumerate(zip(cls_names, classifiers, p_grid)):
  print("\nALGORITHM: ", name)

  fold, models, scores = 1, [], []

  for train, test in skf.split(X, y):
    print("\nFOLD:", fold)
    
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X[train])
    X_test = scaler.transform(X[test])

    gridCV = GridSearchCV(clf(), param_grid=p_grid[grid], cv=5, scoring='accuracy', n_jobs=-1)
    gridCV.fit(X_train, y[train].ravel())

    print("VALIDATION score:", gridCV.best_score_)
    print("BEST parameters:", gridCV.best_params_)

    y_pred = gridCV.predict(X_test)
    y_test = y[test]

    print(classification_report(y_test, y_pred))
    print(confusion_matrix(y_test, y_pred))

    acc = accuracy_score(y_test, y_pred)
    print("TEST score:", str(acc))
    print()

    models.append({"name": name,
                   "validation_score":  gridCV.best_score_,
                   "params":            gridCV.best_params_,
                   "test_score":        str(acc),
                   "report":            classification_report(y_test, y_pred),
                   "confusion_matrix":  confusion_matrix(y_test, y_pred)})
    
    scores.append(gridCV.best_score_)

    fold += 1

  best_model = models[np.argmax(scores)]
  best_score = scores[np.argmax(scores)]

  print("\nBEST MODEL\n")
  print("Validation score: ", best_model["validation_score"])
  print("Params: ",           best_model["params"])
  print("Test score: ",       best_model["test_score"])
  print("Report\n",           best_model["report"])
  print("Confusion matrix\n", best_model["confusion_matrix"])

  best_models.append(best_model)
  best_scores.append(best_score)

"""Si illustri il modello che ha performato meglio nella model selction."""

best_model = best_models[np.argmax(best_scores)]

print("Name: ",             best_model["name"])
print("Validation score: ", best_model["validation_score"])
print("Params: ",           best_model["params"])
print("Test score: ",       best_model["test_score"])
print("Report\n",           best_model["report"])
print("Confusion matrix\n", best_model["confusion_matrix"])

"""Si illustrino i modelli che hanno performato meglio nella model selction."""

for best_model in best_models:
  print("Name: ",             best_model["name"])
  print("Validation score: ", best_model["validation_score"])
  print("Params: ",           best_model["params"])
  print("Test score: ",       best_model["test_score"])
  print("Report\n",           best_model["report"])
  print("Confusion matrix\n", best_model["confusion_matrix"])

"""### **Model selection per `BaggingClassifier`**

Si effettui la model selection per individuare quale sia il miglior weak learner da utilizzare per `BaggingClassifier`.

Si crei un dizionario di dizionari che contiene la griglia dei parametri per `BaggingClassifier`. Il parametro `n_estimators` definisce il numero di stimatori che verranno utilizzati per creare l'ensemble learner.
"""

p_grid = {
    "bc"  : [{'n_estimators': [20, 30, 50, 100, 150], 'warm_start': [False, True], 'random_state': [42], 'n_jobs': [-1]}],
}
p_grid

"""Bagging con stimatori `DecisionTree`."""

best_models, best_scores = [], []

fold, models, scores = 1, [], []

for train, test in skf.split(X, y):
  print("\nFOLD:", fold)
  
  scaler = StandardScaler()
  X_train = scaler.fit_transform(X[train])
  X_test = scaler.transform(X[test])

  gridCV = GridSearchCV(BaggingClassifier(
      base_estimator=DecisionTreeClassifier(max_depth=7, 
                                            criterion='gini', 
                                            ccp_alpha=0.001, 
                                            splitter='best', 
                                            random_state=42)), 
      param_grid=p_grid["bc"], cv=5, scoring='accuracy', n_jobs=-1)
  gridCV.fit(X_train, y[train].ravel())

  print("VALIDATION score:", gridCV.best_score_)
  print("BEST parameters:", gridCV.best_params_)

  y_pred = gridCV.predict(X_test)
  y_test = y[test]

  print(classification_report(y_test, y_pred))
  print(confusion_matrix(y_test, y_pred))

  acc = accuracy_score(y_test, y_pred)
  print("TEST score:", str(acc))
  print()

  models.append({"name": name,
                  "validation_score":  gridCV.best_score_,
                  "params":            gridCV.best_params_,
                  "test_score":        str(acc),
                  "report":            classification_report(y_test, y_pred),
                  "confusion_matrix":  confusion_matrix(y_test, y_pred)})
  
  scores.append(gridCV.best_score_)

  fold += 1

best_model = models[np.argmax(scores)]
best_score = scores[np.argmax(scores)]

print("\nBEST MODEL\n")
print("Validation score: ", best_model["validation_score"])
print("Params: ",           best_model["params"])
print("Test score: ",       best_model["test_score"])
print("Report\n",           best_model["report"])
print("Confusion matrix\n", best_model["confusion_matrix"])

best_models.append(best_model)
best_scores.append(best_score)

"""Bagging con stimatori `K-Nearest Neighbors`."""

best_models, best_scores = [], []

fold, models, scores = 1, [], []

for train, test in skf.split(X, y):
  print("\nFOLD:", fold)
  
  scaler = StandardScaler()
  X_train = scaler.fit_transform(X[train])
  X_test = scaler.transform(X[test])

  gridCV = GridSearchCV(BaggingClassifier(
      base_estimator=KNeighborsClassifier(algorithm='auto', 
                                          leaf_size=20, 
                                          n_neighbors=9, 
                                          weights='uniform', 
                                          n_jobs=-1)), 
      param_grid=p_grid["bc"], cv=5, scoring='accuracy', n_jobs=-1)
  gridCV.fit(X_train, y[train].ravel())

  print("VALIDATION score:", gridCV.best_score_)
  print("BEST parameters:", gridCV.best_params_)

  y_pred = gridCV.predict(X_test)
  y_test = y[test]

  print(classification_report(y_test, y_pred))
  print(confusion_matrix(y_test, y_pred))

  acc = accuracy_score(y_test, y_pred)
  print("TEST score:", str(acc))
  print()

  models.append({"name": name,
                  "validation_score":  gridCV.best_score_,
                  "params":            gridCV.best_params_,
                  "test_score":        str(acc),
                  "report":            classification_report(y_test, y_pred),
                  "confusion_matrix":  confusion_matrix(y_test, y_pred)})
  
  scores.append(gridCV.best_score_)

  fold += 1

best_model = models[np.argmax(scores)]
best_score = scores[np.argmax(scores)]

print("\nBEST MODEL\n")
print("Validation score: ", best_model["validation_score"])
print("Params: ",           best_model["params"])
print("Test score: ",       best_model["test_score"])
print("Report\n",           best_model["report"])
print("Confusion matrix\n", best_model["confusion_matrix"])

best_models.append(best_model)
best_scores.append(best_score)

"""Bagging con stimatori `LogisticRegression`."""

best_models, best_scores = [], []

fold, models, scores = 1, [], []

for train, test in skf.split(X, y):
  print("\nFOLD:", fold)
  
  scaler = StandardScaler()
  X_train = scaler.fit_transform(X[train])
  X_test = scaler.transform(X[test])

  gridCV = GridSearchCV(BaggingClassifier(
      base_estimator=LogisticRegression(C=1, 
                                        max_iter=100, 
                                        warm_start=True, 
                                        random_state=42, 
                                        n_jobs=-1)), 
      param_grid=p_grid["bc"], cv=5, scoring='accuracy', n_jobs=-1)
  gridCV.fit(X_train, y[train].ravel())

  print("VALIDATION score:", gridCV.best_score_)
  print("BEST parameters:", gridCV.best_params_)

  y_pred = gridCV.predict(X_test)
  y_test = y[test]

  print(classification_report(y_test, y_pred))
  print(confusion_matrix(y_test, y_pred))

  acc = accuracy_score(y_test, y_pred)
  print("TEST score:", str(acc))
  print()

  models.append({"name": name,
                  "validation_score":  gridCV.best_score_,
                  "params":            gridCV.best_params_,
                  "test_score":        str(acc),
                  "report":            classification_report(y_test, y_pred),
                  "confusion_matrix":  confusion_matrix(y_test, y_pred)})
  
  scores.append(gridCV.best_score_)

  fold += 1

best_model = models[np.argmax(scores)]
best_score = scores[np.argmax(scores)]

print("\nBEST MODEL\n")
print("Validation score: ", best_model["validation_score"])
print("Params: ",           best_model["params"])
print("Test score: ",       best_model["test_score"])
print("Report\n",           best_model["report"])
print("Confusion matrix\n", best_model["confusion_matrix"])

best_models.append(best_model)
best_scores.append(best_score)

"""## Test dei classificatori

In questa sezione, verranno testati i modelli ottenuti nella fase di model selection.
"""

X = df.iloc[:, df.columns != "target"].values
y = df.iloc[:, df.columns == "target"].values

encoder = LabelEncoder()
y = encoder.fit_transform(y)

encoder.classes_

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""### **Decision Tree**"""

dt = DecisionTreeClassifier(max_depth=7, criterion='gini', ccp_alpha=0.001, splitter='best', random_state=42)

dt.fit(X_train, y_train)

y_pred = dt.predict(X_test)

evaluate(y_test, y_pred)

"""### **K-Nearest Neighbors**"""

knn = KNeighborsClassifier(algorithm='auto', leaf_size=20, n_neighbors=9, weights='uniform', n_jobs=-1)

knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

evaluate(y_test, y_pred)

"""### **Logistic Regression**"""

lgr = LogisticRegression(C=1, max_iter=100, warm_start=True, random_state=42, n_jobs=-1)

lgr.fit(X_train, y_train)

y_pred = lgr.predict(X_test)

evaluate(y_test, y_pred)

"""### **BaggingClassifier**

#### **Bagging con DecisionTree**
"""

bc_dt = BaggingClassifier(dt, n_estimators=20, warm_start=False, random_state=42, n_jobs=-1)

bc_dt.fit(X_train, y_train)

y_pred = bc_dt.predict(X_test)

evaluate(y_test, y_pred)

"""#### **Bagging con K-Nearest Neighbors**"""

bc_knn = BaggingClassifier(knn, n_estimators=30, warm_start=False, random_state=42, n_jobs=-1)

bc_knn.fit(X_train, y_train)

y_pred = bc_knn.predict(X_test)

evaluate(y_test, y_pred)

"""#### **Bagging con Logistic Regression**"""

bc_lgr = BaggingClassifier(lgr, n_estimators=100, warm_start=False, random_state=42, n_jobs=-1)

bc_lgr.fit(X_train, y_train)

y_pred = bc_lgr.predict(X_test)

evaluate(y_test, y_pred)

"""### **RandomForest**"""

rfc = RandomForestClassifier(n_estimators=150, criterion='gini', ccp_alpha=0, warm_start=False, random_state=42, n_jobs=-1)

rfc.fit(X_train, y_train)

y_pred = rfc.predict(X_test)

evaluate(y_test, y_pred)

"""## Conclusioni

Di seguito si illustrino i risultati finali dei vari classificatori addestrati.
"""

print(tabulate([['DecisionTree', accuracy_score(y_test, dt.predict(X_test))], 
                ['K-Nearest Neighbors', accuracy_score(y_test, knn.predict(X_test))],
                ['Logistic Regression', accuracy_score(y_test, lgr.predict(X_test))],
                ['Bagging con DecisionTree', accuracy_score(y_test, bc_dt.predict(X_test))],
                ['Bagging con K-Nearest Neighbors', accuracy_score(y_test, bc_knn.predict(X_test))],
                ['Bagging con Logistic Regression', accuracy_score(y_test, bc_lgr.predict(X_test))],
                ['RandomForest', accuracy_score(y_test, rfc.predict(X_test))]], headers=['Classifier', 'Accuracy'], tablefmt='orgtbl'))

"""Complessivamente è possibile notare che il metodo ensemble ha migliorato la classificazione rispetto alla classificazione effettuata dai singoli *weak learner* o al limite ha prodotto un'accuratezza uguale. Si può vedere che il bagging con `DecisionTree` è migliorato rispetto a `DecisionTree` di circa lo 0.88%.

In conclusione, il classificatore che ha ottenutto l'accuratezza migliore è il bagging con `LogisticRegression` (97.37% circa).
"""