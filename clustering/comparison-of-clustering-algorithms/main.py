# -*- coding: utf-8 -*-
"""Confronto tra gli algoritmi di clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Eje2F3pvMOL8ukVKtIoeVaDPaKILfEjV

# **Confronto tra gli algoritmi di clustering**

## Introduzione

### **Clustering**

Il **clustering** è il processo di *raggruppamento* di un insieme di oggetti in gruppi di oggetti simili. È la tecnica di analisi più comune tra le tecniche di *apprendimento non supervisionato*. Diversamente dall'apprendimento supervisionato, non sono presenti le classificazioni dell'insieme degli esempi (insieme di training). L'abilità delle tecniche di clustering sta proprio nel trovare delle *somiglianze* tra gli oggetti di uno stesso gruppo. In diversi approci, il concetto di similarità viene rappresentato come una *distanza* in uno spazio multidimensionale. Quindi, gli algoritmi di clustering raggruppano gli oggetti sulla base della loro distanza reciproca (il concetto di *similarità*). Quindi l'appartenenza o meno ad un insieme dipende da quanto l'oggetto preso in considerazione è distante dal cluster stesso.

### **Definizione del problema del clustering**

Si definiscano gli elementi coinvolti nella formulazione del problema del clustering:
* Un insieme di esempi: $\mathcal{X} = \{x_1, \dots , x_n\}, x_i \in \mathcal{X}$;
* Una misura di *similarità*;
* Un numero desiderato di cluster $\mathcal{K}$;

Bisogna calcolare la *funzione di assegnamento* $\gamma: \mathcal{X} \rightarrow \{1, \dots , \mathcal{K}\}$. Tale funzione fa in modo che ogni cluster contenga almeno un elemento al suo interno. I cluster vengono etichettati *simbolicamente* con il solo scopo di identificarli: non c'è alcuna relazione d'ordine tra i vari cluster realizzati dall'algoritmo.

Il problema del clustering è un problema *intrinsecamente mal posto*. La nozione di cluster è vaga e arbitraria: uno stesso oggetto può essere classificato per caratteristiche diverse dell'oggetto stesso. In base alla caratteristica scelta, l'oggetto può essere incluso in un gruppo piuttosto che in un altro. È da precisare inoltre che nessuno dei criteri per il clustering è sbagliato, semplicemente ci sono più modi per raggruppare un certo insieme di oggetti.

### **Fasi da effettuare in preparazione al clustering**

Vi sono principalmente due quesiti che devono essere definiti prima di eseguire un algoritmo di clustering:
* **Decidere una rappresentazione degli oggetti**: è una fase fondamentale in quanto la scelta di rappresentare gli oggetti in un dato modo piuttosto che in un altro, può produrre dei cluster differenti. La scelta della rappresentazione deve essere fatta sulla base anche della conoscenza a priori che si dispone. In questo passo è importante decidere qual è la misura di similarità che dovrà essere utilizzata dall'algoritmo di clustering;
* **Decidere il numero di cluster che devono essere considerati**: per decidere il numero di cluster è possibile sfruttare la conoscenza a priori che si dispone. In caso contrario, si cerca di provare *empiricamente* quale sia un ragionevole $\mathcal{K}$. Si cerca di trovare un valore tale per cui non permetta di costruire dei cluster troppo grandi o troppo piccoli. In entrambi i casi risulta difficile analizzare le caratteristiche per cui tali oggetti sono stati raggruppati nel medesimo cluster.

### **Funzione obiettivo**

In certi casi, l'obiettivo di un algoritmo di clustering è quello di ottimizzare una certa *funzione obiettivo*. Il problema del clustering viene inteso come un problema di *ottimizzazione*, con l'obiettivo di minimizzare l'errore prodotto da tale funzione. È comune utilizzare la *distanza*.

La possibilità di attuare una ricerca completa su $k$ cluster viene subito scartata in quanto è molto complessa e può funzionare soltanto in casi semplici (è un problema NP-Hard). Le possibili soluzioni sono $\frac{\mathcal{K}^n}{\mathcal{K}!}$. Per ovviare a questa problematica, gli algoritmi di clustering **partizionali** e **gerarchici** utilizzano degli approci diversi per produrre dei cluster via via più raffiniati.

### **Dataset**

Il dataset utilizzato in questo notebook è il dataset *A3* della sezione *A-Sets*. ([http://cs.joensuu.fi/sipu/datasets/](http://cs.joensuu.fi/sipu/datasets/)).

## Caricamento del dataset e delle librerie necessarie
"""

import numpy as np
import pandas as pd

from sklearn.cluster import KMeans
import scipy.cluster.hierarchy as h
from sklearn.metrics.cluster import adjusted_rand_score, adjusted_mutual_info_score

import matplotlib.pyplot as plt
from itertools import cycle

np.random.seed(42)

"""Si effettui il caricamento del dataset."""

X = pd.read_csv("a3.txt", sep="   ", header=None).values
y = pd.read_csv("a3-ga.pa", skiprows=4, header=None).values

X

X.shape

y

y.shape

"""## Algoritmi partizionali

### **Introduzione agli algortimi partizionali**

Questi algoritmi creano una **partizione** degli esempi forniti, con l'obiettivo di minimizzare $\sum_{i = 1}^\mathcal{K} \gamma(C_i)$, dove $\gamma$ è la funzione di costo associata al singolo cluster, $\mathcal{K}$ è il numero di cluster e $C_i$ è l'$i$-esimo cluster. L'algoritmo più conosciuto di questa categoria è *K-Means*.

### **Introduzione all'algoritmo K-Means**

L'assunzione iniziale è che gli esempi siano vettori a valori reali. Per ogni cluster si minimizzi la media della distanza tra gli esempi e il *centro* del cluster, detto **centroide** ($c$): $\mu(c) = \frac{1}{|c|} \sum_{x \in c} x$

I vari oggetti vengono assegnati ai cluster in base alla similarità/distanza degli esempi dai centroidi dei cluster correnti.

### **Implementazione dell'algoritmo K-Means**

Si descrivino i passi dell'algoritmo:
1. Si generino $\mathcal{K}$ punti nello spazio. Questi punti rappresentano i *centroidi iniziali*;
2. Si assegni ad ogni esempio al cluster il cui centroide è il più vicino secondo la similarità/distanza considerata;
3. Dopo aver assegnato tutti gli esempi, si ricalcolino le posizioni dei $\mathcal{K}$ centroidi come medie dei vettori degli esempi appartenenti ai rispettivi cluster;
4. Si ripetano i passi 2 e 3 fino a quando i centroidi si stabilizzano.

Di seguito, si è proceduti all'implementazione di una classe che implementa i passi appena descritti. Unica variazione attuata è il *criterio di terminazione* dell'algortimo: l'algortimo termina quando i centroidi si stabilizzano o quando si è raggiunto un certo numero di iterazioni, normalmente impostato a 300 (`max_iter`).

Sono stati definiti anche dei metodi per la valutazione dei risultati prodotti dall'algoritmo di clustering. **Rand-Index**: è un concetto simile alla nozione di *accuratezza* utilizzata nella classificazione. Per ogni coppia di esempi, si consideri se essi siano stati correttamente distribuiti nei cluster, ovvero, risultano nello stesso cluster *se e solo se* sono della stessa classe nel *ground truth*, che in questo caso è rappresentato da `y`. Per il calcolo del RandIndex si definiscano:
* $A$: numero di coppie della stessa classe assegnate allo stesso cluster (**true positive**);
* $B$: numero di coppie di classe diversa assegnate allo stesso cluster (**false positive**);
* $C$: numero di coppie della stessa classe assegnate a cluster diversi (**false negative**);
* $D$: numero di coppie di classe diversa assegnate a cluster diversi (**true negative**).

Quindi, RandIndex è definito come:
\begin{equation}
  RI = \frac{A + D}{A + B + C + D}
\end{equation}

È possibile inoltre definire delle misure corrispondenti a *precision* e *recall*:
\begin{equation}
  P = \frac{A}{A + B}
\end{equation}

\begin{equation}
  R = \frac{A}{A + C}
\end{equation}
"""

class K_Means:

  def __init__(self, n_clusters=8, max_iter=300, random_state=0):
    # Setup the seed for the random generator
    np.random.seed(random_state)
    self.n_clusters = n_clusters
    self.max_iter = max_iter

  def fit(self, X):

    # Step 1: generate K points (seeds) in the space. 
    #         These points are the initial centroids.
    old_centroids = np.zeros(shape=(self.n_clusters, X.shape[1]), dtype=int)
    new_centroids = np.array([X [np.random.choice(X.shape[0])] for k in range(self.n_clusters)])

    clusters = [[]]
    i = 0
    y_pred = []

    # Termination condition:  the new centroids are the same as the old ones 
    #                         OR 
    #                         the maximum number of iterations has been reached.
    while not np.array_equal(new_centroids, old_centroids) and i < self.max_iter:

      # Let n_clusters = c. Prepare c empty clusters. 
      clusters = [[] for c in range(self.n_clusters)]
      y_pred = []

      # Step 2: For each x in X, assign x to the cluster whose centroid is the 
      #         closest according to the considered similarity (distance)
      for x in X:
          minimum_distance = float('inf')
          index = -1

          # Find the index of the nearest centroid for x.
          for cluster_index in range(self.n_clusters):
            candidate_centroid = new_centroids[cluster_index]
            current_distance = self.distance(x, candidate_centroid)

            if current_distance < minimum_distance:
                minimum_distance = current_distance
                index = cluster_index

          clusters[index].append(x)
          y_pred.append(index)

      # Step 3: recompute the positions of centroids as means of the vectors of 
      #         the example, belonging to the respective clusters.
      old_centroids = new_centroids
      new_centroids = np.array([sum(cluster) / len(cluster) for cluster in clusters])

      i += 1
      
    return clusters, y_pred, new_centroids

  def predict(self, X, clusters, centroids):
    y_pred = []

    for x in X:
      minimum_distance = float('inf')
      index = -1

      for cluster_index in range(len(clusters)):
        current_distance = self.distance(x, centroids[cluster_index])

        if current_distance < minimum_distance:
            minimum_distance = current_distance
            index = cluster_index

      y_pred.append(index)

    return np.array([[pred] for pred in y_pred])

  # Define the similarity measure (norm 2).
  def distance(self, x, y):
    return np.sqrt(np.sum((np.array(x) - np.array(y)) ** 2))

  def rand_index(self, y_test, y_pred):
    a = self.true_positive(y_test, y_pred)
    b = self.false_positive(y_test, y_pred)
    c = self.false_negative(y_test, y_pred)
    d = self.true_negative(y_test, y_pred)
    
    return (a + d) / (a + b + c + d)

  def true_positive(self, y_test, y_pred):
    tp = 0
    for i in range(len(y_pred)):
          k = i + 1

          while (k < len(y_pred)):
              if y_test[i] == y_test[k] and y_pred[i] == y_pred[k]:
                  tp += 1

              k += 1
    return tp

  def true_negative(self, y_test, y_pred):
    tn = 0

    for i in range(len(y_pred)):
        k = i + 1

        while (k < len(y_pred)):
            if y_test[i] != y_test[k] and y_pred[i] != y_pred[k]:
                tn += 1

            k += 1
    return tn

  def false_positive(self, y_test, y_pred):
    fp = 0

    for i in range(len(y_pred)):
        k = i + 1

        while (k < len(y_pred)):
            if y_test[i] != y_test[k] and y_pred[i] == y_pred[k]:
                fp += 1

            k += 1
    return fp

  def false_negative(self, y_test, y_pred):
    fn = 0

    for i in range(len(y_pred)):
        k = i + 1

        while (k < len(y_pred)):
            if y_test[i] == y_test[k] and y_pred[i] != y_pred[k]:
                fn += 1

            k += 1
    return fn

  def precision(self, y_test, y_pred):
    a = self.true_positive(y_test, y_pred)
    b = self.false_positive(y_test, y_pred)

    return a / (a + b)

  def recall(self, y_test, y_pred):
    a = self.true_positive(y_test, y_pred)
    c = self.false_negative(y_test, y_pred)
    
    return a / (a + c)

"""Si inizializzi l'algoritmo K-Means."""

model = K_Means(n_clusters=50, max_iter=1000, random_state=42)

"""Si determinino i vari cluster."""

clusters, y_fit, centroids = model.fit(X)

"""Si definiscano alcune valutazioni dei cluster prodotti."""

model.rand_index(y.flatten(), y_fit)

model.precision(y.flatten(), y_fit)

model.recall(y.flatten(), y_fit)

adjusted_rand_score(y.flatten(), y_fit)

adjusted_mutual_info_score(y.flatten(), y_fit)

"""Si illustrino graficamente i vari cluster. Per i cluster che hanno lo stesso colore non vuol dire che appartengono al medesimo cluster. I vari cluster sono identificati dal loro numero."""

def plot_clusters(clusters, centroids):
  fig, ax = plt.subplots(figsize=(15,10))

  for i in range(len(clusters)):
      x, y = list(zip(*[(coord[0], coord[1]) for coord in clusters[i]]))
      ax.scatter(x, y, marker='.', linewidth=1.5)
  
  for i, c in enumerate(centroids):
    ax.scatter(c[0], c[1], c='black', linewidth=1)
    ax.annotate(i, (c[0], c[1]))

plot_clusters(clusters, centroids)

"""### **K-Means con sklearn**

Si inizializzi l'algoritmo K-Means.
"""

kmeans = KMeans(n_clusters=50, max_iter=1000, random_state=42, n_jobs=-1)

"""Si determinino i vari cluster."""

kmeans.fit(X)

"""Si definiscano alcune valutazioni dei cluster prodotti."""

model.rand_index(y.flatten(), kmeans.labels_)

model.precision(y.flatten(), kmeans.labels_)

model.recall(y.flatten(), kmeans.labels_)

adjusted_rand_score(y.flatten(), kmeans.labels_)

adjusted_mutual_info_score(y.flatten(), kmeans.labels_)

"""## Algoritmi gerarchici

### **Introduzione agli algoritmi gerarchici**

Quando il numero di cluster $\mathcal{K}$ ottimale non è fornito, gli algoritmi di questa categoria sono una buona alternativa. Tali algoritmi costruiscono una *tassonomia gerachica* da un insieme di esempi che rappresenta la struttura dei cluster: il **dendogramma**.

Questi algoritmi sono a loro volta suddivisi in due classi:
* **Agglomerativo** (**bottom-up)**: questi algoritmi assumono che inizialmente ogni cluster contenga un singolo oggetto. Ad ogni passo vengono fusi i cluster più *vicini* fino ad ottenere un singolo grande cluster. Questi algoritmi necessitano di misure per valutare la similarità tra cluster, per scegliere la coppia di cluster da fondere ad ogni passo;
* **Divisivo** (**top-down**): questi algoritmi partono da un singolo grande cluster contenente tutti gli oggetti. Ad ogni passo viene diviso in base ad una certa misura. Solitamente viene fissato un numero minimo di punti sotto il quale il cluster non viene ulteriormente suddiviso (nel caso estremo questo valore è 1). Questi tipi di algoritmi necessitano di definire una funzione per scegliere il cluster da suddividere.

In questo notebook si utilizzeranno degli algoritmi gerarchici agglomerativi.

### **Single-Link**

La similarità tra due cluster è intesa come la similarità tra gli oggetti più simili. Utilizzando tale criterio, si raggrupperrano quegli oggetti che avranno, di conseguenza, una distanza minore rispetto ad altri oggetti. Poiché il criterio di unione è *strettamente locale*, una *catena* di esempi può essere estesa per lunghe distanze senza tenere conto della forma complessiva del cluster che si sta andando a creare. Questo effetto è chiamato *chaining*.
"""

result = h.single(X)

plt.figure(figsize=(15, 10))
h.dendrogram(result)
plt.show()

flat_single = h.fcluster(result, 1394, criterion='distance')

adjusted_rand_score(y.flatten(), flat_single)

adjusted_mutual_info_score(y.flatten(), flat_single)

"""### **Complete-Link**

Questa misura di similarità permette di raggruppare degli oggetti che sono invece il più possibile distanti fra loro. Di conseguenza, questo vuol dire che tale criterio andrà a raggruppare oggetti che saranno *diversi* tra loro. Questo criterio presenta una problematica, ovvero, è soggeto agli *outlier*.
"""

result = h.complete(X)

plt.figure(figsize=(15, 10))
h.dendrogram(result)
plt.show()

flat_single = h.fcluster(result, 5588, criterion='distance')

adjusted_rand_score(y.flatten(), flat_single)

adjusted_mutual_info_score(y.flatten(), flat_single)

"""### **Average-Link (Group-Link)**

Questa misura di similarità calcola la distanza tra i due cluster come la media delle distanze tra i singoli elementi. Questo criterio rappresenta una soluzione intermedia tra il *single-link* e il *complete-link*.
"""

result = h.average(X)

plt.figure(figsize=(15, 10))
h.dendrogram(result)
plt.show()

flat_single = h.fcluster(result, 1394, criterion='distance')

adjusted_rand_score(y.flatten(), flat_single)

adjusted_mutual_info_score(y.flatten(), flat_single)

"""### **Centroid**

Per ogni cluster viene calcolato un *centroide* che rappresenta la media. I cluster vengono uniti in base a i centroidi più simili tra loro. Tali cluster vengono uniti a due a due.
"""

result = h.centroid(X)

plt.figure(figsize=(15, 10))
h.dendrogram(result)
plt.show()

flat_single = h.fcluster(result, 1394, criterion='distance')

adjusted_rand_score(y.flatten(), flat_single)

adjusted_mutual_info_score(y.flatten(), flat_single)

"""## Conclusioni

In conclusione si è visto che per questo dataset K-Means ha prodotto dei cluster molto più raffinati rispetto agli algoritmi gerarchici. Vantaggio per l'algoritmo K-Means è che si conosceva a priori il numero di cluster che dovevano essere creati.

L'implementazione dell'algoritmo K-Means non è prestante rispetto all'implementazione di *sklearn*, tuttavia producono dei risultati simili.

Per quanto riguarda gli algoritmi gerarchici, l'algoritmo che ha performato meglio è stato *single-link*. Tuttavia, vi è una differenza nell'accuratezza rispetto a K-Means.
"""